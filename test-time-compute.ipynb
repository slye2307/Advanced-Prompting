{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Solve Challenging Problems using Advanced Prompt Engineering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow and steady wins the race! This is true for many things, including LLM model outputs. While fast inference is desirable in certain situations like building real-time chatbots or live translation apps, we want our models to take their time and reason deliberately through complex problems. For example, when conducting scientific research, analyzing financial data for investment decisions, or solving challenging mathematical proofs, accuracy matters far more than speed—and it's worth allocating extra compute time to get the answer right.\n",
    "\n",
    "In this project, you'll learn about Test-Time Compute by experimenting with how strategies to increase compute can improve results for reasoning through mathematical problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#What-is-Test-Time-Compute?\">What is Test-Time Compute?</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Chain-of-Thought-(CoT)-Prompting\">Chain-of-Thought (CoT) Prompting</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#CoT-Experiment-Results\">CoT Experiment Results</a>\n",
    "    </li>\n",
    "    <li><a href=\"#Best-of-N-Sampling\">Best-of-N Sampling</li>\n",
    "    <li><a href=\"#Best-of-N-Experiment-Results\">Best-of-N Experiment Results</a></li>\n",
    "    <li><a href=\"#Self-verification-Strategy\">Self-verification Strategy</a></li>\n",
    "    <li><a href=\"#Self-verification-Experiment-Results\">Self-verification Experiment Results</a></li>\n",
    "    <li>\n",
    "         <a href=\"#Exercises\">Exercises</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Exercise-1---Implementing-Chain-of-Thought-Prompting\">Exercise 1 - Implementing Chain-of-Thought Prompting</a></li>\n",
    "            <li><a href=\"#Exercise-2---Best-of-N-Sampling-with-Custom-Selection\">Exercise 2 - Best-of-N Sampling with Custom Selection</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "* Explain the concept of **Test-Time Compute** and its role in improving model performance during inference.\n",
    "* Apply **Chain-of-Thought prompting** to enable step-by-step reasoning in large language models.\n",
    "* Use **Best-of-N sampling** to generate multiple candidate outputs and select the most accurate one.\n",
    "* Implement **Self-verification** strategies to allow models to check and refine their own answers.\n",
    "* Analyze how increasing test-time compute affects solution accuracy, consistency, and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`langchain`](https://www.langchain.com/) for running and managing inference with OpenAI models.\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Installing the libraries may take up to 10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed jiter-0.13.0 langchain-core-0.3.83 langchain-openai-0.3.30 langsmith-0.7.4 openai-1.109.1 orjson-3.11.7 regex-2026.1.15 requests-toolbelt-1.0.0 tiktoken-0.12.0 uuid-utils-0.14.0 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed langchain-0.3.27 langchain-text-splitters-0.3.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai==0.3.30  | tail -n 1\n",
    "%pip install langchain==0.3.27 | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: running this next cell will cause a pop up notification, but it's nothing to worry about! Just close it and keep progressing through the project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import get_ipython \n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Test-Time Compute? \n",
    "\n",
    "**Test-Time Compute** refers to the use of additional computational resources (time, tokens, reasoning steps) during inference when an LLM is answering a query. The main idea is that giving models more \"time to think\" at inference can improve their performance on complex problems. \n",
    "\n",
    "The traditional approach to inference has been the following: the model reads the prompt → generates the answer immediately (with an emphasis increasing speed of inference). The **Test-Time Compute** approach is: the model reads the prompt → engages in extended reasoning → produces a better answer.\n",
    "\n",
    "This can involve several different strategies:\n",
    "\n",
    "* **Chain-of-Thought Prompting**: step-by-step problem solving\n",
    "* **Best of N Sampling**: generating several solutions and selecting the best\n",
    "* **Self-verification**: checking and correcting its own work\n",
    "* **Search/tree exploration**: exploring multiple solution paths\n",
    "\n",
    "In this lab, we'll experiment with them all to test how increased compute improves results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain of thought prompting is basically just asking an AI to \"show its work\" - like a math teacher would ask you to do in school. \n",
    "\n",
    "Instead of jumping straight to an answer, you prompt the AI to walk through its reasoning step-by-step. So instead of asking \"what's 47 x 23?\" and getting just \"1081\", you'd ask it to break down the problem, such that the response might look like \"First, I'll multiply 47 by 20, which gives me 940. Then 47 by 3, which is 141. Adding those together: 940 + 141 = 1081.\"\n",
    "\n",
    "The cool thing is that this actually makes AI models better at complex problems. When they articulate their reasoning, they're less likely to make mistakes, especially on tricky logic puzzles, math problems, or anything requiring multiple steps. \n",
    "\n",
    "How does this relate back to Test-Time Compute? Essentially, you can make an LLM smarter by training it on more data, OR you can let it spend more time reasoning through a problem. Chain of Thought is one way to use that extra compute as the model generates more tokens as it thinks through steps, which means more computation happening, which often leads to better answers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT Chain of Thought:\n",
      "\n",
      "\n",
      "The sum is 54.\n",
      "\n",
      "============================================================\n",
      "\n",
      "WITH Chain of Thought:\n",
      "\n",
      "\n",
      "Step 1: Reverse the digits of 27\n",
      "27 reversed is 72.\n",
      "\n",
      "Step 2: Add 27 and 72\n",
      "27 + 72 = 99\n",
      "\n",
      "Therefore, the sum of 27 and its reversed digits is 99.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(name=\"gpt-5-mini\", temperature=0.3)\n",
    "\n",
    "problem = \"A number is formed by reversing the digits of 27. Add this number to 27. What’s the sum?\"\n",
    "\n",
    "# Without CoT\n",
    "print(\"WITHOUT Chain of Thought:\")\n",
    "response_direct = llm.invoke(f\"Answer directly: {problem}\")\n",
    "print(response_direct)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\") # print formatting \n",
    "\n",
    "# With CoT\n",
    "print(\"WITH Chain of Thought:\")\n",
    "cot_prompt = f\"\"\"Think step-by-step and show your reasoning:\n",
    "{problem}\"\"\"\n",
    "response_cot = llm.invoke(cot_prompt)\n",
    "print(response_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoT Experiment Results\n",
    "\n",
    "As you can see above, without the CoT prompt to think step-by-step and show the model's reasoning, the model immediately responds with the incorrect answer of 54, presumably because it just added 27 + 27. However, when the model takes the extra time to write out the reverse of the number 27, it correctly adds 27 + 72 which is 99. \n",
    "\n",
    "Here's what's really interesting about this example: it shows that the model isn't actually \"thinking\" differently in some hidden way - the act of writing out the intermediate steps literally changes the computation.\n",
    "It's almost like the model can't \"see\" what it needs to do until it externalizes it. By forcing itself to type \"the reverse of 27 is 72,\" it creates new information in its context that it can then use. Without that step written down, it's like the reversal never happened in the model's \"mind.\"\n",
    "This also hints at why test-time compute matters so much. When the model generates more tokens (the reverse step, the addition breakdown, etc.), it's essentially giving itself more opportunities to correct course. Each token it outputs becomes part of the context for the next token, creating a sort of scaffold for better reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best-of-N Sampling \n",
    "\n",
    "With Best-of-N sampling, we generate multiple answers to the same question and pick the best one. \n",
    "\n",
    "Essentially, instead of making a single call to the LLM, you ask it the same question N times (5 or 10 times). Each time, the model might approach the problem slightly differently or make different choices in its reasoning. Some answers will be correct, some might have mistakes. \n",
    "\n",
    "Then, you select the best answer from all the candidates. You can do this by:\n",
    "- Picking the most common answer (majority vote)\n",
    "- Using human verification to check which answer is correct\n",
    "- Having another LLM judge which response is highest quality\n",
    "\n",
    "The underlying principle is that the model is more likely to get things right than wrong but correctness isn't always 100% reliable. If you generate 10 answers and 8 of them say \"42\" while 2 say \"41\", then the answer is probably 42. \n",
    "\n",
    "This is the Test-Time Compute principle in practice. You're using more computational resources during inference (running the model multiple times instead of once) to improve the quality of the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Best-of-N Sampling (N=7)\n",
      "============================================================\n",
      "Problem: \n",
      "A farmer has 10 cats. Half of them run away, and 3 return. Then half of the remaining ones have kittens, 2 each. How many cats are there now?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(name=\"gpt-5-mini\", temperature=0.7)  # higher temp for variety\n",
    "\n",
    "# a problem where we want to sample multiple solutions\n",
    "problem = \"\"\"\n",
    "A farmer has 10 cats. Half of them run away, and 3 return. Then half of the remaining ones have kittens, 2 each. How many cats are there now?\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Best-of-N Sampling (N=7)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Problem: {problem}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Multiple Candidate Solutions\n",
    "\n",
    "This step generates multiple independent solutions to the same problem by calling the language model N times (in this case, 7 times) with identical prompts. By producing several candidate answers, the system creates a diverse pool of potential solutions that may use different reasoning approaches or arrive at different conclusions, which can then be compared and evaluated to identify the most reliable answer through consensus or quality assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating candidates...\n",
      "\n",
      "Candidate 1:\n",
      "\n",
      "Step 1: Start with 10 cats.\n",
      "Step 2: Half of them run away, so 10 divided by 2 is 5. There are now 5 cats remaining.\n",
      "Step 3: 3 of the cats return, so add 3 to the current number of cats (5) to get 5+3=8 cats.\n",
      "Step 4: Half of the remaining cats have kittens, so half of 8 is 4. This means that 4 cats have kittens, and each cat has 2 kittens. 4 x 2 = 8 kittens.\n",
      "Step 5: Add the number of kittens (8) to the current number of cats (8) to get 8+8=16 cats in total.\n",
      "Therefore, after all the changes, there are now 16 cats. \n",
      "------------------------------------------------------------\n",
      "Candidate 2:\n",
      "\n",
      "Step 1: Start with the given information.\n",
      "The farmer has 10 cats.\n",
      "\n",
      "Step 2: Half of the cats run away.\n",
      "Half of 10 is 5.\n",
      "There are now 10 - 5 = 5 cats left.\n",
      "\n",
      "Step 3: 3 of the cats return.\n",
      "There are now 5 + 3 = 8 cats.\n",
      "\n",
      "Step 4: Half of the remaining cats have kittens.\n",
      "Half of 8 is 4.\n",
      "There are now 8 - 4 = 4 cats without kittens.\n",
      "\n",
      "Step 5: Each of the remaining cats have 2 kittens.\n",
      "4 cats x 2 kittens each = 8 kittens.\n",
      "There are now 4 + 8 = 12 cats in total.\n",
      "\n",
      "Step 6: Final answer.\n",
      "There are now 12 cats on the farm.\n",
      "------------------------------------------------------------\n",
      "Candidate 3:\n",
      "\n",
      "1. Start with the initial number of cats: 10\n",
      "2. Half of them run away, so we are left with 10/2 = 5 cats.\n",
      "3. 3 cats return, so we add 3 to the remaining 5 cats: 5 + 3 = 8 cats.\n",
      "4. Half of the remaining cats (8/2) have kittens, so we multiply 8/2 by 2 to find the total number of cats with kittens: (8/2) x 2 = 8 cats with kittens.\n",
      "5. Each of these 8 cats have 2 kittens each, so we multiply 8 x 2 = 16 kittens.\n",
      "6. Finally, we add the 8 cats with kittens and the 16 kittens to find the total number of cats: 8 + 16 = 24 cats.\n",
      "Therefore, there are now 24 cats on the farm.\n",
      "------------------------------------------------------------\n",
      "Candidate 4:\n",
      "\n",
      "1. Start with the given information: The farmer has 10 cats.\n",
      "\n",
      "2. Half of the cats run away, so we divide 10 by 2 to find out how many cats run away: 10 ÷ 2 = 5 cats run away.\n",
      "\n",
      "3. Subtract the number of cats that run away from the original 10 cats: 10 - 5 = 5 cats remain.\n",
      "\n",
      "4. Three of the cats that ran away return, so we add 3 to the remaining 5 cats: 5 + 3 = 8 cats remain.\n",
      "\n",
      "5. Half of the remaining cats have kittens, so we divide 8 by 2 to find out how many cats have kittens: 8 ÷ 2 = 4 cats have kittens.\n",
      "\n",
      "6. Each of the 4 cats have 2 kittens, so we multiply 4 by 2 to find out how many kittens there are in total: 4 x 2 = 8 kittens.\n",
      "\n",
      "7. Add the number of remaining cats and kittens to find the total number of cats: 8 + 8 = 16 cats in total.\n",
      "\n",
      "Therefore, there are 16 cats now.\n",
      "------------------------------------------------------------\n",
      "Candidate 5:\n",
      "\n",
      "Step 1: Start with 10 cats.\n",
      "\n",
      "Step 2: Half of them run away. This leaves us with 10/2 = 5 cats.\n",
      "\n",
      "Step 3: 3 cats return. This brings the total number of cats to 5 + 3 = 8.\n",
      "\n",
      "Step 4: Half of the remaining cats have kittens. Half of 8 is 8/2 = 4. This means 4 cats will have kittens.\n",
      "\n",
      "Step 5: Each of the 4 cats has 2 kittens. This adds 4 x 2 = 8 kittens to the total number of cats.\n",
      "\n",
      "Step 6: Add the kittens to the remaining cats. 8 + 8 = 16.\n",
      "\n",
      "Therefore, there are 16 cats now.\n",
      "------------------------------------------------------------\n",
      "Candidate 6:\n",
      "\n",
      "1. Start by writing out the initial number of cats the farmer has: 10 cats.\n",
      "2. Half of the cats run away, so multiply 10 by 1/2 (or divide by 2) to find how many cats are left: 10 x 1/2 = 5 cats.\n",
      "3. Next, 3 of the cats return, so we add 3 to the current number of cats: 5 + 3 = 8 cats.\n",
      "4. Half of the remaining cats have kittens, so multiply the current number of cats (8) by 1/2: 8 x 1/2 = 4 cats.\n",
      "5. Each of these 4 cats has 2 kittens, so we multiply 4 by 2 to find the total number of kittens: 4 x 2 = 8 kittens.\n",
      "6. Finally, we add the number of kittens (8) to the current number of cats (8): 8 + 8 = 16 cats.\n",
      "7. Therefore, there are now 16 cats on the farm.\n",
      "------------------------------------------------------------\n",
      "Candidate 7:\n",
      "\n",
      "Step 1: Start with the initial number of cats, which is 10.\n",
      "Step 2: Half of them run away, so divide 10 by 2 to get 5.\n",
      "Step 3: 3 cats return, so add 3 to the remaining 5 cats to get 8.\n",
      "Step 4: Half of the remaining cats (8) have kittens, so divide 8 by 2 to get 4.\n",
      "Step 5: Each of these 4 cats has 2 kittens, so multiply 4 by 2 to get 8.\n",
      "Step 6: Add the 8 kittens to the 4 remaining cats to get a total of 12 cats.\n",
      "Therefore, there are now 12 cats on the farm. \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# generate N candidates\n",
    "N = 7\n",
    "candidates = []\n",
    "\n",
    "print(\"Generating candidates...\\n\")\n",
    "for i in range(N):\n",
    "    response = llm.invoke(f\"Solve this step-by-step:\\n{problem}\")\n",
    "    candidates.append(response)\n",
    "    print(f\"Candidate {i+1}:\")\n",
    "    print(response)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Best Answer Through Voting\n",
    "\n",
    "This code below extracts the final numerical answer from each candidate solution by using a regular expression to find all whole numbers in the response text and taking the last number as the assumed answer. By collecting all the final answers into a list, the system prepares to implement a voting mechanism where the most frequently occurring answer across all candidates is selected as the most reliable solution, based on the assumption that multiple independent reasoning paths converging on the same answer increases confidence in its correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Selecting best answer...\n",
      "============================================================\n",
      "\n",
      "All answers: ['16', '12', '24', '16', '16', '16', '12']\n"
     ]
    }
   ],
   "source": [
    "# simple voting/selection mechanism\n",
    "# count how many times each answer appears\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Selecting best answer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# extract final numbers from each candidate (simple approach)\n",
    "final_answers = []\n",
    "for candidate in candidates:\n",
    "    # look for numbers in the response\n",
    "    # finds all whole numbers in the string \n",
    "    # (sequences of digits surrounded by word boundaries)\n",
    "    numbers = re.findall(r'\\b\\d+\\b', candidate)\n",
    "    if numbers:\n",
    "        final_answers.append(numbers[-1])  # Take the last number as the answer\n",
    "\n",
    "print(f\"\\nAll answers: {final_answers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Most Common Answer\n",
    "\n",
    "The final bit of code below uses Python's `Counter` class to tally how many times each numerical answer appears across all candidates, then identifies the most frequently occurring answer as the best solution. It displays this consensus answer along with how many candidates agreed on it, and prints out the full reasoning from the first candidate that arrived at this most common answer, providing both the final result and a complete solution path that led to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common answer: 16 (appeared 4/7 times)\n",
      "\n",
      "Best candidate (most common answer):\n",
      "\n",
      "Step 1: Start with 10 cats.\n",
      "Step 2: Half of them run away, so 10 divided by 2 is 5. There are now 5 cats remaining.\n",
      "Step 3: 3 of the cats return, so add 3 to the current number of cats (5) to get 5+3=8 cats.\n",
      "Step 4: Half of the remaining cats have kittens, so half of 8 is 4. This means that 4 cats have kittens, and each cat has 2 kittens. 4 x 2 = 8 kittens.\n",
      "Step 5: Add the number of kittens (8) to the current number of cats (8) to get 8+8=16 cats in total.\n",
      "Therefore, after all the changes, there are now 16 cats. \n"
     ]
    }
   ],
   "source": [
    "# find most common answer using collections.Counter\n",
    "# to count the frequencies of the final answers\n",
    "answer_counts = Counter(final_answers)\n",
    "most_common_answer = answer_counts.most_common(1)[0]\n",
    "\n",
    "print(f\"\\nMost common answer: {most_common_answer[0]} (appeared {most_common_answer[1]}/{N} times)\")\n",
    "print(f\"\\nBest candidate (most common answer):\")\n",
    "# Print the first candidate with the most common answer\n",
    "for i, ans in enumerate(final_answers):\n",
    "    if ans == most_common_answer[0]:\n",
    "        print(candidates[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best-of-N Experiment Results\n",
    "\n",
    "As you can see above with each inference call, there was some variation in the reasoning paths and even the final answers. By collecting N independent inference samples from the model, and selecting the most consistent final answer, we're using more computational resources to improve the accuracy of our results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Self-verification Strategy\n",
    "\n",
    "Self-verification is when you ask the LLM to check its own work, the same way a student might review their test answers before turning it in. In a lot of cases for both LLMs or students, they may have made a mistake on their first attempt, and upon checking over their work, find the error and are able to fix it if they take the time to do so.\n",
    "\n",
    "Here's the process: \n",
    "1. **Generate an initial answer**: ask the model to solve a problem\n",
    "2. **Verify the answer**: ask the model to review its own solution and check for errors\n",
    "3. **Refine if needed**: based on the verification, produce a corrected final answer\n",
    "\n",
    "The main idea is giving the model a chance to catch its own mistakes. This is another form of test-time compute. You're running the model multiple times on the same problem - once to solve it, once to verify it, and possibly once more to refine it. That's more computational work than a single-pass answer, but it often leads to more accurate results. You're trading compute for quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Self-Verification Strategy\n",
      "============================================================\n",
      "Problem: \n",
      "problem = \"A number is formed by reversing the digits of 27. Add this number to 27. What’s the sum?\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set-up\n",
    "llm = OpenAI(name=\"gpt-5-mini\", temperature=0.3)\n",
    "\n",
    "problem = \"\"\"\n",
    "problem = \"A number is formed by reversing the digits of 27. Add this number to 27. What’s the sum?\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Self-Verification Strategy\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Problem: {problem}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Model Gets Its Initial Response\n",
    "\n",
    "The code sends a straightforward prompt to the language model using `llm.invoke()`, which combines the instruction \"Solve this problem:\" with the actual problem text. The model processes this prompt in a single pass and returns its first attempt at a solution without any self-reflection or verification - this initial answer may contain errors or incomplete reasoning that will be refined in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Initial Answer\n",
      "------------------------------------------------------------\n",
      "\n",
      "The number formed by reversing the digits of 27 is 72. Adding this number to 27 results in a sum of 99. Therefore, the sum is 99.\n"
     ]
    }
   ],
   "source": [
    "# getting the initial answer\n",
    "\n",
    "print(\"STEP 1: Initial Answer\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "initial_prompt = f\"Solve this problem:\\n{problem}\"\n",
    "initial_answer = llm.invoke(initial_prompt)\n",
    "print(initial_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Verification Step\n",
    "\n",
    "The verification prompt asks the model to act as its own critic by presenting the original problem alongside the initial answer and requesting a thorough review. The model checks the solution by examining the logical reasoning for flaws, verifying that any calculations are performed correctly, and ensuring the final answer is sensible in the context of the problem - essentially forcing the model to step back and evaluate its own work from a fresh perspective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Self-Verification\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. The logic of the solution is correct. The problem asks to reverse the digits of 27, which results in the number 72. Adding this number to 27 gives a sum of 99.\n",
      "2. The calculations are also correct. 27 + 72 = 99.\n",
      "3. The final answer of 99 makes sense as it is the sum of two numbers (27 and 72) that are formed by the same digits, but in reverse order. This is a common pattern in math and the answer aligns with this pattern. \n"
     ]
    }
   ],
   "source": [
    "# using self-verification\n",
    "\n",
    "print(\"STEP 2: Self-Verification\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "verification_prompt = f\"\"\"\n",
    "Here's a problem and a proposed solution. Check if the solution is correct.\n",
    "If you find any errors, explain what's wrong and provide the correct answer.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Proposed Solution:\n",
    "{initial_answer}\n",
    "\n",
    "Is this solution correct? Verify by:\n",
    "1. Checking the logic\n",
    "2. Verifying the calculations\n",
    "3. Confirming the final answer makes sense\n",
    "\"\"\"\n",
    "\n",
    "verification = llm.invoke(verification_prompt)\n",
    "print(verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Refinement Step\n",
    "\n",
    "The refinement prompt provides the model with the complete context: the original problem, its initial attempt, and the verification feedback identifying any errors or confirming correctness. By synthesizing all three pieces of information, the model can incorporate the insights from the verification step to either correct mistakes found in the initial answer or confidently reaffirm the solution if it was already correct, producing a final refined response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Final Refined Answer\n",
      "------------------------------------------------------------\n",
      "\n",
      "The final, correct answer is 99.\n"
     ]
    }
   ],
   "source": [
    "# getting the final refined answer\n",
    "\n",
    "print(\"STEP 3: Final Refined Answer\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "refinement_prompt = f\"\"\"\n",
    "Based on this verification, provide the final correct answer:\n",
    "\n",
    "Original problem: {problem}\n",
    "\n",
    "Initial attempt: {initial_answer}\n",
    "\n",
    "Verification: {verification}\n",
    "\n",
    "What is the final, correct answer?\n",
    "\"\"\"\n",
    "\n",
    "final_answer = llm.invoke(refinement_prompt)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-verification Experiment Results\n",
    "\n",
    "As we can see above in our verification strategy experiment, when we make a second call to the LLM to run a self-verification check, it reasons through and sometimes finds that the initial answer to the problem was incorrect, and corrects the answer. \n",
    "\n",
    "The verification prompt forces more careful, step-by-step reasoning by breaking down the problem into explicit sub-questions. This structured approach helps the model catch errors that it made during the initial approach. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now let's extend what we've learned with some hands-on exercises: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Implementing Chain-of-Thought Prompting \n",
    "\n",
    "In the lesson, we saw how Chain-of-Thought prompting helps models reason through problems step-by-step. Now it's your turn to implement a CoT prompt from scratch! A useful thing to know given how often we use LLMs as tools. \n",
    "\n",
    "**Instructions**: Complete the code below to implement a Chain-of-Thought prompting function. The function should guide the model through a structured reasoning process for math word problems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_solve(problem: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    Solve a problem using Chain-of-Thought prompting.\n",
    "    \n",
    "    Args:\n",
    "        problem (str): The problem to solve\n",
    "        llm: The language model instance\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response with step-by-step reasoning\n",
    "    \"\"\"\n",
    "    # TODO 1: Create a prompt that asks the model to solve the problem step-by-step\n",
    "    # Your prompt should explicitly instruct the model to:\n",
    "    # - Break down the problem\n",
    "    # - Show each calculation step\n",
    "    # - Provide a final answer\n",
    "    \n",
    "    cot_prompt = \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 2: Invoke the LLM with your Chain-of-Thought prompt\n",
    "    response = # YOUR CODE HERE\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test your implementation\n",
    "problem = \"Sarah has 3 boxes. Each box contains 4 bags. Each bag has 5 marbles. How many marbles does Sarah have in total?\"\n",
    "result = chain_of_thought_solve(problem, llm)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "    \n",
    "```python\n",
    "def chain_of_thought_solve(problem: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    Solve a problem using Chain-of-Thought prompting.\n",
    "    Args:\n",
    "    problem (str): The problem to solve\n",
    "    llm: The language model instance\n",
    "    \n",
    "    Returns:\n",
    "    str: The model's response with step-by-step reasoning\n",
    "    \"\"\"\n",
    "    cot_prompt = f\"\"\"\n",
    "    Solve the following problem step-by-step. Show your reasoning at each step.\n",
    "\n",
    "    Problem: {problem}\n",
    "\n",
    "    Please:\n",
    "    1. Break down what information is given\n",
    "    2. Identify what needs to be calculated\n",
    "    3. Show each calculation step with explanation\n",
    "    4. Provide the final answer\n",
    "\n",
    "    Let's think through this step by step:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(cot_prompt)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Best-of-N Sampling with Custom Selection \n",
    "\n",
    "In this exercise, you'll implement a Best-of-N sampling strategy with a custom scoring function to select the best answer. \n",
    "\n",
    "**Instructions**: Complete the function below to generate N candidate solutions and select the best one based on a scoring criteria. You'll need to implement both the generation loop and selection logic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n_sample(problem: str, llm, n: int = 3, temperature: float = 0.7) -> dict:\n",
    "    \"\"\"\n",
    "    Generate N solutions and return the best one based on confidence scoring.\n",
    "    \n",
    "    Args:\n",
    "        problem (str): The problem to solve\n",
    "        llm: The language model instance\n",
    "        n (int): Number of candidate solutions to generate\n",
    "        temperature (float): Temperature for sampling diversity\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with 'best_answer', 'all_answers', and 'scores'\n",
    "    \"\"\"\n",
    "    # TODO 1: Create a prompt that asks the model to solve the problem\n",
    "    # and rate its confidence (0-10) in the answer\n",
    "    prompt = f\"\"\"\n",
    "    # YOUR CODE HERE - Create a prompt that:\n",
    "    # 1. Asks to solve: {problem}\n",
    "    # 2. Requests a confidence score (0-10)\n",
    "    # 3. Uses a format like \"Answer: X\\nConfidence: Y\"\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # TODO 2: Generate N candidate solutions\n",
    "    # Hint: You'll need to call llm.invoke() N times\n",
    "    for i in range(n):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    # TODO 3: Parse confidence scores from each candidate\n",
    "    # Extract the confidence score from responses (look for \"Confidence: X\")\n",
    "    scores = []\n",
    "    for candidate in candidates:\n",
    "        # YOUR CODE HERE - extract confidence score\n",
    "        # Hint: You might use string methods like .split() or regex\n",
    "        pass\n",
    "    \n",
    "    # TODO 4: Select the candidate with the highest confidence score\n",
    "    best_idx = # YOUR CODE HERE\n",
    "    \n",
    "    return {\n",
    "        'best_answer': candidates[best_idx],\n",
    "        'all_answers': candidates,\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "# Test your implementation\n",
    "problem = \"What is 15% of 240?\"\n",
    "result = best_of_n_sample(problem, llm, n=3)\n",
    "print(\"Best Answer:\", result['best_answer'])\n",
    "print(\"\\nAll Scores:\", result['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "    \n",
    "```python\n",
    "def best_of_n_sample(problem: str, llm, n: int = 3, temperature: float = 0.7) -> dict:\n",
    "    \"\"\"\n",
    "    Generate N solutions and return the best one based on confidence scoring.\n",
    "\n",
    "    Args:\n",
    "    problem (str): The problem to solve\n",
    "    llm: The language model instance\n",
    "    n (int): Number of candidate solutions to generate\n",
    "    temperature (float): Temperature for sampling diversity\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with 'best_answer', 'all_answers', and 'scores'\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Solve this problem: {problem}\n",
    "\n",
    "    After providing your answer, rate your confidence in the solution on a scale of 0-10.\n",
    "\n",
    "    Format your response as:\n",
    "    Answer: [your solution]\n",
    "    Confidence: [0-10]\n",
    "    \"\"\"\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # Generate N candidate solutions\n",
    "    for i in range(n):\n",
    "        response = llm.invoke(prompt)\n",
    "        candidates.append(response)\n",
    "\n",
    "    # Parse confidence scores from each candidate\n",
    "    scores = []\n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            # Extract confidence score\n",
    "            confidence_line = [line for line in candidate.split('\\n') if 'Confidence:' in line][0]\n",
    "            score = float(confidence_line.split(':')[1].strip())\n",
    "            scores.append(score)\n",
    "        except:\n",
    "            scores.append(0)  # Default score if parsing fails\n",
    "\n",
    "    # Select the candidate with the highest confidence score\n",
    "    best_idx = scores.index(max(scores))\n",
    "\n",
    "    return {\n",
    "        'best_answer': candidates[best_idx],\n",
    "        'all_answers': candidates,\n",
    "        'scores': scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary \n",
    "\n",
    "Congratulations! You’ve explored the powerful concept of **Test-Time Compute (TTC)** and how it enhances reasoning and accuracy in large language models during inference. In this lab, you learned how to:\n",
    "\n",
    "* **Explain Test-Time Compute**: Understand how allocating more compute at inference can improve reasoning quality.\n",
    "* **Apply Chain-of-Thought Prompting**: Enable models to reason step-by-step through complex problems.\n",
    "* **Use Best-of-N Sampling**: Generate multiple candidate responses and select the most accurate or consistent one.\n",
    "* **Implement Self-Verification**: Allow models to review, critique, and refine their own outputs.\n",
    "* **Analyze Performance Trade-offs**: Evaluate how increasing test-time compute impacts accuracy, consistency, and efficiency.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "There's still a lot we can improve upon! If you're up for a challenge, take a look at the below next steps to continue learning:\n",
    "\n",
    "* Experiment with **search-based reasoning** (e.g., Tree of Thoughts) to explore multiple reasoning paths.\n",
    "* Integrate **self-consistency scoring** or **ensemble reasoning** to further refine outputs.\n",
    "* Visualize reasoning traces to understand how the model’s internal thought process evolves with more compute.\n",
    "* Compare efficiency metrics (latency vs. accuracy) to find the optimal compute budget for your task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Congrats on completing this project image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/vg_icJK9Wf6TUzkeM0FZfw/IMG-0640.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tenzin Migmar](https://author.skills.network/instructors/tenzin_migmar): Hi, I'm Tenzin. I'm a data scientist intern at IBM interested in applying machine learning to solve difficult problems. Prior to joining IBM, I worked as a research assistant on projects exploring perspectivism and personalization within large language models. In my free time, I enjoy recreational programming and learning to cook new recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Abdul Fatir](https://author.skills.network/instructors/abdul_fatir): Abdul specializes in Data Science, Machine Learning, and AI. He has deep expertise in understanding how the latest technologies work, and their applications. Feel free to contact him with questions about this project or any other AI/ML topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2025 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "c044df8032939353a18b2a370d36eeefd189f4e823cadf5efcb81f87eee4faae"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
